# æ–°é—»æºé…ç½®æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•æ·»åŠ å’Œé…ç½®æ–°çš„æ–°é—»æºã€‚

## ğŸ“° å½“å‰ä½¿ç”¨çš„æ–°é—»æº

### 1. Google News RSS
- **URL**: `https://news.google.com/rss/search?q=economy+OR+stock+market+OR+GDP+OR+inflation`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: èšåˆå¤šä¸ªæ–°é—»æºï¼Œå†…å®¹ä¸°å¯Œ
- **é™åˆ¶**: æ— 
- **æ›´æ–°é¢‘ç‡**: å®æ—¶

### 2. Yahoo Finance RSS
- **URL**: `https://finance.yahoo.com/news/rssindex`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: é‡‘èæ–°é—»ä¸“ä¸šï¼Œæ•°æ®å‡†ç¡®
- **é™åˆ¶**: æ— 
- **æ›´æ–°é¢‘ç‡**: å®æ—¶

### 3. BBC Business RSS
- **URL**: `https://feeds.bbci.co.uk/news/business/rss.xml`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: æƒå¨åª’ä½“ï¼Œå›½é™…è§†è§’
- **é™åˆ¶**: æ— 
- **æ›´æ–°é¢‘ç‡**: æ¯å°æ—¶

### 4. CNBC RSS
- **URL**: `https://www.cnbc.com/id/100003114/device/rss/rss.html`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: ç¾å›½è´¢ç»æ–°é—»ï¼Œå¸‚åœºåˆ†ææ·±å…¥
- **é™åˆ¶**: æ— 
- **æ›´æ–°é¢‘ç‡**: å®æ—¶

### 5. Financial Times RSS
- **URL**: `https://www.ft.com/?format=rss`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: é«˜è´¨é‡é‡‘èåˆ†æ
- **é™åˆ¶**: éƒ¨åˆ†å†…å®¹éœ€è¦è®¢é˜…
- **æ›´æ–°é¢‘ç‡**: æ¯å°æ—¶

### 6. Bloomberg RSS
- **URL**: `https://www.bloomberg.com/feed/podcast/etf-report.xml`
- **ç±»å‹**: RSS 2.0
- **ä¼˜ç‚¹**: ä¸“ä¸šé‡‘èæ•°æ®
- **é™åˆ¶**: æ— 
- **æ›´æ–°é¢‘ç‡**: æ¯å°æ—¶

## ğŸ”§ å¦‚ä½•æ·»åŠ æ–°çš„RSSæ–°é—»æº

### æ­¥éª¤1ï¼šæ‰¾åˆ°RSSè®¢é˜…åœ°å€

å¤§å¤šæ•°æ–°é—»ç½‘ç«™éƒ½æä¾›RSSè®¢é˜…ï¼Œé€šå¸¸å¯ä»¥åœ¨é¡µé¢åº•éƒ¨æ‰¾åˆ°RSSå›¾æ ‡æˆ–é“¾æ¥ã€‚

**å¸¸è§RSSè®¢é˜…åœ°å€æ ¼å¼ï¼š**
- `https://example.com/rss`
- `https://example.com/feed`
- `https://example.com/rss.xml`
- `https://example.com/feed.xml`

### æ­¥éª¤2ï¼šä¿®æ”¹çˆ¬è™«è„šæœ¬

ç¼–è¾‘ `scripts/fetch_news.py` æ–‡ä»¶ï¼Œåœ¨ `RSS_FEEDS` åˆ—è¡¨ä¸­æ·»åŠ æ–°çš„è®¢é˜…æºï¼š

```python
RSS_FEEDS = [
    # ... ç°æœ‰çš„è®¢é˜…æº ...
    {
        'url': 'https://your-news-site.com/rss.xml',
        'source': 'æ–°é—»æºåç§°',
        'country': 'å›½å®¶/åœ°åŒº'
    }
]
```

### æ­¥éª¤3ï¼šæµ‹è¯•

åœ¨æœ¬åœ°è¿è¡Œè„šæœ¬æµ‹è¯•ï¼š

```bash
python scripts/fetch_news.py
```

## ğŸŒ æ¨èçš„å…è´¹æ–°é—»æº

### ç»æµç±»
- **Reuters Business**: `https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best`
- **MarketWatch**: `https://www.marketwatch.com/rss/`
- **The Economist**: `https://www.economist.com/rss`

### ç§‘æŠ€ç±»
- **TechCrunch**: `https://techcrunch.com/feed/`
- **The Verge**: `https://www.theverge.com/rss/index.xml`
- **Wired**: `https://www.wired.com/feed/rss`

### åŠ å¯†è´§å¸
- **CoinDesk**: `https://www.coindesk.com/arc/outboundfeeds/rss/`
- **Cointelegraph**: `https://cointelegraph.com/rss`
- **Bitcoin Magazine**: `https://bitcoinmagazine.com/.rss/full/`

### ä¸­æ–‡æ–°é—»æº
- **æ–°æµªè´¢ç»**: `https://finance.sina.com.cn/roll/index.d.html?format=rss`
- **è´¢æ–°ç½‘**: `http://www.caixin.com/rss/rss_index.xml`
- **åå°”è¡—è§é—»**: `https://wallstreetcn.com/rss`

## ğŸ› ï¸ ä½¿ç”¨å…¶ä»–å…è´¹API

### 1. NewsData.io
- **ç½‘ç«™**: https://newsdata.io/
- **å…è´¹é¢åº¦**: æ¯å¤©200æ¬¡è¯·æ±‚
- **æ³¨å†Œ**: éœ€è¦é‚®ç®±æ³¨å†Œ

```python
import requests

API_KEY = 'your_api_key'
url = f'https://newsdata.io/api/1/news?apikey={API_KEY}&q=economy&language=en'
response = requests.get(url)
data = response.json()
```

### 2. Currents API
- **ç½‘ç«™**: https://currentsapi.services/
- **å…è´¹é¢åº¦**: æ¯å¤©600æ¬¡è¯·æ±‚
- **æ³¨å†Œ**: éœ€è¦é‚®ç®±æ³¨å†Œ

```python
import requests

API_KEY = 'your_api_key'
url = f'https://api.currentsapi.services/v1/latest-news?apiKey={API_KEY}&category=business'
response = requests.get(url)
data = response.json()
```

### 3. GNews API
- **ç½‘ç«™**: https://gnews.io/
- **å…è´¹é¢åº¦**: æ¯å¤©100æ¬¡è¯·æ±‚
- **æ³¨å†Œ**: éœ€è¦é‚®ç®±æ³¨å†Œ

```python
import requests

API_KEY = 'your_api_key'
url = f'https://gnews.io/api/v4/search?q=economy&token={API_KEY}&lang=en'
response = requests.get(url)
data = response.json()
```

## ğŸ” ç½‘é¡µçˆ¬å–æ–¹æ¡ˆ

å¦‚æœRSSä¸å¯ç”¨ï¼Œå¯ä»¥ç›´æ¥çˆ¬å–ç½‘é¡µå†…å®¹ï¼š

```python
import requests
from bs4 import BeautifulSoup

def scrape_news_website(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # æ ¹æ®ç½‘ç«™ç»“æ„æå–æ–°é—»
    articles = soup.find_all('article')
    
    news_list = []
    for article in articles:
        title = article.find('h2').text
        link = article.find('a')['href']
        description = article.find('p').text
        
        news_list.append({
            'title': title,
            'url': link,
            'description': description
        })
    
    return news_list
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éµå®ˆrobots.txt**: æ£€æŸ¥ç½‘ç«™çš„ `robots.txt` æ–‡ä»¶ï¼Œç¡®ä¿å…è®¸çˆ¬å–
2. **è¯·æ±‚é¢‘ç‡**: ä¸è¦é¢‘ç¹è¯·æ±‚ï¼Œå»ºè®®æ¯æ¬¡è¯·æ±‚é—´éš”1-2ç§’
3. **User-Agent**: è®¾ç½®åˆç†çš„User-Agentï¼Œé¿å…è¢«è¯†åˆ«ä¸ºæœºå™¨äºº
4. **ç‰ˆæƒé—®é¢˜**: ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸è¦å•†ä¸šä½¿ç”¨
5. **åçˆ¬è™«æœºåˆ¶**: æŸäº›ç½‘ç«™å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œéœ€è¦ä½¿ç”¨ä»£ç†æˆ–å…¶ä»–æ–¹æ³•

## ğŸ”„ æ›´æ–°é¢‘ç‡å»ºè®®

- **å®æ—¶æ–°é—»**: æ¯1-2å°æ—¶æ›´æ–°ä¸€æ¬¡
- **æ·±åº¦åˆ†æ**: æ¯6-12å°æ—¶æ›´æ–°ä¸€æ¬¡
- **å‘¨æŠ¥æœˆæŠ¥**: æ¯å¤©æ›´æ–°ä¸€æ¬¡

## ğŸ“Š æ•°æ®è´¨é‡ä¼˜åŒ–

### å»é‡
```python
seen_urls = set()
unique_news = []
for news in all_news:
    if news['url'] not in seen_urls:
        seen_urls.add(news['url'])
        unique_news.append(news)
```

### è¿‡æ»¤
```python
# è¿‡æ»¤æ‰æ ‡é¢˜å¤ªçŸ­çš„æ–°é—»
filtered_news = [n for n in news_list if len(n['title']) > 20]

# è¿‡æ»¤æ‰ç‰¹å®šå…³é”®è¯
excluded_keywords = ['advertisement', 'sponsored']
filtered_news = [n for n in news_list 
                 if not any(kw in n['title'].lower() for kw in excluded_keywords)]
```

### æ’åº
```python
# æŒ‰å‘å¸ƒæ—¶é—´æ’åº
sorted_news = sorted(news_list, key=lambda x: x['published_at'], reverse=True)

# æŒ‰ç›¸å…³æ€§æ’åº
def calculate_relevance(news):
    keywords = ['economy', 'market', 'stock', 'GDP']
    score = sum(1 for kw in keywords if kw in news['title'].lower())
    return score

sorted_news = sorted(news_list, key=calculate_relevance, reverse=True)
```

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ å‘ç°äº†å¥½ç”¨çš„æ–°é—»æºï¼Œæ¬¢è¿æäº¤PRæˆ–Issueåˆ†äº«ï¼

---

**æœ€åæ›´æ–°**: 2025-10-23
