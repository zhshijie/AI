import requests
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict
import time
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

# Êñ∞ÈóªAPIÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
NEWS_API_KEY = os.getenv('NEWS_API_KEY', '')

# RSSËÆ¢ÈòÖÊ∫êÂàóË°®ÔºàÂÆåÂÖ®ÂÖçË¥πÔºåÊó†ÈúÄAPIÂØÜÈí•Ôºâ
RSS_FEEDS = [
    {
        'url': 'https://feeds.bbci.co.uk/news/business/rss.xml',
        'source': 'BBC Business',
        'country': 'UK'
    },
    {
        'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        'source': 'CNBC',
        'country': 'USA'
    },
    {
        'url': 'https://www.ft.com/?format=rss',
        'source': 'Financial Times',
        'country': 'UK'
    },
    {
        'url': 'https://www.bloomberg.com/feed/podcast/etf-report.xml',
        'source': 'Bloomberg',
        'country': 'USA'
    }
]

def fetch_from_rss(feed_url: str, source: str, country: str) -> List[Dict]:
    """‰ªéRSSËÆ¢ÈòÖÊ∫êËé∑ÂèñÊñ∞Èóª"""
    news_list = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(feed_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            # Ëß£ÊûêRSS XML
            root = ET.fromstring(response.content)
            
            # Êü•ÊâæÊâÄÊúâitemÊ†áÁ≠æ
            items = root.findall('.//item')
            
            for item in items[:10]:  # ÊØè‰∏™Ê∫êÂèñ10Êù°
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    # Ê∏ÖÁêÜHTMLÊ†áÁ≠æ
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text().strip()
                    
                    # ËΩ¨Êç¢Êó•ÊúüÊ†ºÂºè
                    try:
                        if pub_date:
                            pub_date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                            pub_date = pub_date_obj.isoformat()
                    except:
                        pub_date = datetime.now().isoformat()
                    
                    news_item = {
                        'id': hash(link),
                        'title': title,
                        'description': description[:300] if description else '',
                        'url': link,
                        'source': source,
                        'published_at': pub_date,
                        'category': categorize_news(title + ' ' + description),
                        'country': country,
                        'image_url': get_placeholder_image(),
                        'fetched_at': datetime.now().isoformat()
                    }
                    
                    news_list.append(news_item)
                except Exception as e:
                    print(f"Ëß£ÊûêRSSÈ°πÁõÆÊó∂Âá∫Èîô: {str(e)}")
                    continue
            
            print(f"‰ªé {source} Ëé∑ÂèñÂà∞ {len(news_list)} Êù°Êñ∞Èóª")
            
    except Exception as e:
        print(f"Ëé∑ÂèñRSSËÆ¢ÈòÖÊ∫êÂ§±Ë¥• ({source}): {str(e)}")
    
    return news_list

def fetch_from_google_news() -> List[Dict]:
    """‰ªéGoogle NewsÁà¨ÂèñÁªèÊµéÊñ∞Èóª"""
    news_list = []
    
    try:
        # Google News RSS - ÂïÜ‰∏öÁ±ªÂà´
        url = 'https://news.google.com/rss/search?q=economy+OR+stock+market+OR+GDP+OR+inflation&hl=en-US&gl=US&ceid=US:en'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
            
            for item in items[:15]:  # Âèñ15Êù°
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    # Ê∏ÖÁêÜHTML
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text().strip()
                    
                    # ËΩ¨Êç¢Êó•Êúü
                    try:
                        if pub_date:
                            pub_date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                            pub_date = pub_date_obj.isoformat()
                    except:
                        pub_date = datetime.now().isoformat()
                    
                    news_item = {
                        'id': hash(link),
                        'title': title,
                        'description': description[:300] if description else '',
                        'url': link,
                        'source': 'Google News',
                        'published_at': pub_date,
                        'category': categorize_news(title + ' ' + description),
                        'country': 'Global',
                        'image_url': get_placeholder_image(),
                        'fetched_at': datetime.now().isoformat()
                    }
                    
                    news_list.append(news_item)
                except Exception as e:
                    print(f"Ëß£ÊûêGoogle NewsÈ°πÁõÆÊó∂Âá∫Èîô: {str(e)}")
                    continue
            
            print(f"‰ªé Google News Ëé∑ÂèñÂà∞ {len(news_list)} Êù°Êñ∞Èóª")
            
    except Exception as e:
        print(f"Ëé∑ÂèñGoogle NewsÂ§±Ë¥•: {str(e)}")
    
    return news_list

def fetch_from_yahoo_finance() -> List[Dict]:
    """‰ªéYahoo FinanceÁà¨ÂèñÊñ∞Èóª"""
    news_list = []
    
    try:
        # Yahoo Finance RSS
        url = 'https://finance.yahoo.com/news/rssindex'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
            
            for item in items[:10]:
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text().strip()
                    
                    try:
                        if pub_date:
                            pub_date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')
                            pub_date = pub_date_obj.isoformat()
                    except:
                        pub_date = datetime.now().isoformat()
                    
                    news_item = {
                        'id': hash(link),
                        'title': title,
                        'description': description[:300] if description else '',
                        'url': link,
                        'source': 'Yahoo Finance',
                        'published_at': pub_date,
                        'category': categorize_news(title + ' ' + description),
                        'country': 'USA',
                        'image_url': get_placeholder_image(),
                        'fetched_at': datetime.now().isoformat()
                    }
                    
                    news_list.append(news_item)
                except Exception as e:
                    print(f"Ëß£ÊûêYahoo FinanceÈ°πÁõÆÊó∂Âá∫Èîô: {str(e)}")
                    continue
            
            print(f"‰ªé Yahoo Finance Ëé∑ÂèñÂà∞ {len(news_list)} Êù°Êñ∞Èóª")
            
    except Exception as e:
        print(f"Ëé∑ÂèñYahoo FinanceÂ§±Ë¥•: {str(e)}")
    
    return news_list

def fetch_economic_news() -> List[Dict]:
    """‰ªéÂ§ö‰∏™Êù•Ê∫êËé∑ÂèñÁªèÊµéÊñ∞Èóª"""
    print("ÂºÄÂßãËé∑ÂèñÁªèÊµéÊñ∞Èóª...")
    
    all_news = []
    
    # 1. ‰ªéGoogle NewsËé∑Âèñ
    print("\n[1/3] ‰ªé Google News Ëé∑Âèñ...")
    google_news = fetch_from_google_news()
    all_news.extend(google_news)
    time.sleep(2)
    
    # 2. ‰ªéYahoo FinanceËé∑Âèñ
    print("\n[2/3] ‰ªé Yahoo Finance Ëé∑Âèñ...")
    yahoo_news = fetch_from_yahoo_finance()
    all_news.extend(yahoo_news)
    time.sleep(2)
    
    # 3. ‰ªéRSSËÆ¢ÈòÖÊ∫êËé∑Âèñ
    print("\n[3/3] ‰ªé RSS ËÆ¢ÈòÖÊ∫êËé∑Âèñ...")
    for feed in RSS_FEEDS:
        try:
            rss_news = fetch_from_rss(feed['url'], feed['source'], feed['country'])
            all_news.extend(rss_news)
            time.sleep(1)
        except Exception as e:
            print(f"RSSËÆ¢ÈòÖÊ∫ê {feed['source']} Â§±Ë¥•: {str(e)}")
            continue
    
    # ÂéªÈáçÔºàÂü∫‰∫éURLÔºâ
    seen_urls = set()
    unique_news = []
    for news in all_news:
        if news['url'] not in seen_urls:
            seen_urls.add(news['url'])
            unique_news.append(news)
    
    # ÊåâÂèëÂ∏ÉÊó∂Èó¥ÊéíÂ∫è
    unique_news.sort(key=lambda x: x['published_at'], reverse=True)
    
    print(f"\nÊÄªÂÖ±Ëé∑Âèñ {len(unique_news)} Êù°ÂéªÈáçÂêéÁöÑÊñ∞Èóª")
    return unique_news

def categorize_news(text: str) -> str:
    """Ê†πÊçÆÊñáÊú¨ÂÜÖÂÆπÂàÜÁ±ªÊñ∞Èóª"""
    text_lower = text.lower()
    
    if any(word in text_lower for word in ['fed', 'central bank', 'interest rate', 'monetary', 'ecb', 'pboc']):
        return 'Ë¥ßÂ∏ÅÊîøÁ≠ñ'
    elif any(word in text_lower for word in ['gdp', 'growth', 'inflation', 'unemployment', 'cpi', 'ppi']):
        return 'ÁªèÊµéÊï∞ÊçÆ'
    elif any(word in text_lower for word in ['stock', 'market', 'trading', 'shares', 'dow', 'nasdaq', 's&p']):
        return 'ËÇ°Â∏ÇÂä®ÊÄÅ'
    elif any(word in text_lower for word in ['oil', 'gold', 'commodity', 'crude', 'silver', 'copper']):
        return 'Â§ßÂÆóÂïÜÂìÅ'
    elif any(word in text_lower for word in ['currency', 'forex', 'exchange rate', 'dollar', 'euro', 'yuan']):
        return 'Â§ñÊ±áÂ∏ÇÂú∫'
    elif any(word in text_lower for word in ['crypto', 'bitcoin', 'blockchain', 'ethereum']):
        return 'Êï∞Â≠óË¥ßÂ∏Å'
    elif any(word in text_lower for word in ['trade', 'tariff', 'export', 'import', 'wto']):
        return 'ÂõΩÈôÖË¥∏Êòì'
    else:
        return 'ÁªºÂêàË¥¢Áªè'

def get_placeholder_image() -> str:
    """Ëé∑ÂèñÂç†‰ΩçÂõæÁâá"""
    images = [
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/c114dc3c-72a6-44d4-93a8-b97d0608ad58/image_1761210122_4_1.png',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/5f2cdbe0-ff90-4dd6-847d-f845db12d841/image_1761210122_6_1.jpg',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/d73313bb-1da1-4247-8ca6-de693c674018/image_1761210124_5_1.png',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/e21eb98a-5751-440c-8dc6-b04471809fc8/image_1761210124_7_1.png'
    ]
    import random
    return random.choice(images)

def analyze_with_ai(news_list: List[Dict]) -> Dict:
    """‰ΩøÁî®AIÂàÜÊûêÊäïËµÑÊ∏©Â∫¶"""
    print("\nÂºÄÂßãAIÂàÜÊûê...")
    
    # ÁÆÄÂåñÁöÑÊÉÖÊÑüÂàÜÊûêÔºàÂü∫‰∫éÂÖ≥ÈîÆËØçÔºâ
    positive_keywords = [
        'surge', 'rise', 'gain', 'growth', 'increase', 'boost', 'rally',
        'positive', 'optimistic', 'strong', 'recovery', 'improve', 'beat',
        'up', 'higher', 'advance', 'jump', 'soar', 'climb'
    ]
    
    negative_keywords = [
        'fall', 'drop', 'decline', 'decrease', 'loss', 'crash', 'plunge',
        'negative', 'pessimistic', 'weak', 'recession', 'concern', 'miss',
        'down', 'lower', 'tumble', 'sink', 'slump', 'slide'
    ]
    
    positive_count = 0
    negative_count = 0
    neutral_count = 0
    key_factors = []
    
    for news in news_list[:30]:  # ÂàÜÊûêÊúÄÊñ∞30Êù°
        text = (news.get('title', '') + ' ' + news.get('description', '')).lower()
        
        pos_score = sum(1 for word in positive_keywords if word in text)
        neg_score = sum(1 for word in negative_keywords if word in text)
        
        if pos_score > neg_score:
            positive_count += 1
            if len(key_factors) < 5:
                key_factors.append(f"‚úì {news['category']}: {news['title'][:60]}...")
        elif neg_score > pos_score:
            negative_count += 1
            if len(key_factors) < 5:
                key_factors.append(f"‚úó {news['category']}: {news['title'][:60]}...")
        else:
            neutral_count += 1
    
    # ËÆ°ÁÆóÊäïËµÑÊ∏©Â∫¶ÂàÜÊï∞
    total = positive_count + negative_count + neutral_count
    if total > 0:
        temperature_score = ((positive_count * 1.0 + neutral_count * 0.5) / total) * 100
    else:
        temperature_score = 50.0
    
    # Á°ÆÂÆöÊÉÖÁª™ÂíåÂàÜÊûêÊñáÊú¨
    if temperature_score >= 70:
        sentiment = "‰πêËßÇ"
        sentiment_emoji = "üòä"
        analysis_text = f"ÂΩìÂâçÂÖ®ÁêÉÁªèÊµéÊñ∞ÈóªÊï¥‰ΩìÂÅèÂêëÁßØÊûÅÔºàÁßØÊûÅÊñ∞ÈóªÂç†ÊØî{positive_count}/{total}ÔºâÔºåÂ∏ÇÂú∫ÊÉÖÁª™‰πêËßÇ„ÄÇÂ§öÈ°πÁªèÊµéÊåáÊ†áË°®Áé∞ËâØÂ•ΩÔºåÊäïËµÑËÄÖ‰ø°ÂøÉËæÉÂº∫„ÄÇÂª∫ËÆÆÈÄÇÂ∫¶Â¢ûÂä†È£éÈô©ËµÑ‰∫ßÈÖçÁΩÆÔºåÂÖ≥Ê≥®ÁßëÊäÄ„ÄÅÊ∂àË¥πÁ≠âÊàêÈïøÊÄßÊùøÂùó„ÄÇ"
    elif temperature_score >= 50:
        sentiment = "‰∏≠ÊÄß"
        sentiment_emoji = "üòê"
        analysis_text = f"ÂΩìÂâçÂÖ®ÁêÉÁªèÊµéÊñ∞ÈóªÂñúÂøßÂèÇÂçäÔºàÁßØÊûÅ{positive_count}Êù°ÔºåÊ∂àÊûÅ{negative_count}Êù°ÔºâÔºåÂ∏ÇÂú∫ÊÉÖÁª™Áõ∏ÂØπ‰∏≠ÊÄß„ÄÇÁªèÊµéÊï∞ÊçÆÊúâÂ•ΩÊúâÂùèÔºåÂª∫ËÆÆ‰øùÊåÅÂùáË°°ÈÖçÁΩÆÔºåÂÖ≥Ê≥®Â∏ÇÂú∫ÂèòÂåñÔºåÈÄÇÊó∂Ë∞ÉÊï¥ÊäïËµÑÁªÑÂêà„ÄÇ"
    else:
        sentiment = "Ë∞®ÊÖé"
        sentiment_emoji = "üòü"
        analysis_text = f"ÂΩìÂâçÂÖ®ÁêÉÁªèÊµéÊñ∞ÈóªÂÅèÂêëÊ∂àÊûÅÔºàÊ∂àÊûÅÊñ∞ÈóªÂç†ÊØî{negative_count}/{total}ÔºâÔºåÂ∏ÇÂú∫Â≠òÂú®ËæÉÂ§ö‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂª∫ËÆÆÈôç‰ΩéÈ£éÈô©ÊïûÂè£ÔºåÂ¢ûÂä†Èò≤Âæ°ÊÄßËµÑ‰∫ßÈÖçÁΩÆÔºåÂ¶ÇÂÄ∫Âà∏„ÄÅÈªÑÈáëÁ≠âÈÅøÈô©ËµÑ‰∫ß„ÄÇ"
    
    analysis_result = {
        'temperature_score': round(temperature_score, 2),
        'sentiment': sentiment,
        'sentiment_emoji': sentiment_emoji,
        'analysis_text': analysis_text,
        'key_factors': key_factors,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'neutral_count': neutral_count,
        'analyzed_news_count': total,
        'analyzed_at': datetime.now().isoformat(),
        'categories_distribution': get_category_distribution(news_list)
    }
    
    print(f"ÂàÜÊûêÂÆåÊàê: Ê∏©Â∫¶={temperature_score:.1f}¬∞, ÊÉÖÁª™={sentiment}")
    return analysis_result

def get_category_distribution(news_list: List[Dict]) -> Dict[str, int]:
    """ÁªüËÆ°Êñ∞ÈóªÂàÜÁ±ªÂàÜÂ∏É"""
    distribution = {}
    for news in news_list:
        category = news.get('category', 'ÂÖ∂‰ªñ')
        distribution[category] = distribution.get(category, 0) + 1
    return distribution

def save_data(news_list: List[Dict], analysis: Dict):
    """‰øùÂ≠òÊï∞ÊçÆÂà∞JSONÊñá‰ª∂"""
    os.makedirs('data', exist_ok=True)
    
    # ‰øùÂ≠òÊñ∞ÈóªÊï∞ÊçÆ
    with open('data/news.json', 'w', encoding='utf-8') as f:
        json.dump({
            'updated_at': datetime.now().isoformat(),
            'total_count': len(news_list),
            'news': news_list
        }, f, ensure_ascii=False, indent=2)
    
    # ‰øùÂ≠òÂàÜÊûêÁªìÊûú
    with open('data/analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print("\n‚úÖ Êï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞ data/ ÁõÆÂΩï")

def main():
    """‰∏ªÂáΩÊï∞"""
    print("=" * 60)
    print("ÂÖ®ÁêÉÁªèÊµéÊñ∞ÈóªÁà¨Âèñ‰∏éÂàÜÊûê‰ªªÂä°")
    print(f"ÊâßË°åÊó∂Èó¥: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Ëé∑ÂèñÊñ∞Èóª
    news_list = fetch_economic_news()
    
    if not news_list:
        print("\n‚ö†Ô∏è  Êú™Ëé∑ÂèñÂà∞Êñ∞ÈóªÊï∞ÊçÆÔºå‰ΩøÁî®Ê®°ÊãüÊï∞ÊçÆ")
        news_list = generate_mock_news()
    
    # AIÂàÜÊûê
    analysis = analyze_with_ai(news_list)
    
    # ‰øùÂ≠òÊï∞ÊçÆ
    save_data(news_list, analysis)
    
    print("\n" + "=" * 60)
    print("‚úÖ ‰ªªÂä°ÂÆåÊàê!")
    print(f"üì∞ Ëé∑ÂèñÊñ∞Èóª: {len(news_list)} Êù°")
    print(f"üå°Ô∏è  ÊäïËµÑÊ∏©Â∫¶: {analysis['temperature_score']:.1f}¬∞")
    print(f"üòä Â∏ÇÂú∫ÊÉÖÁª™: {analysis['sentiment']}")
    print("=" * 60)

def generate_mock_news() -> List[Dict]:
    """ÁîüÊàêÊ®°ÊãüÊñ∞ÈóªÊï∞ÊçÆÔºàÂ§áÁî®Ôºâ"""
    mock_news = [
        {
            'id': 1,
            'title': 'Federal Reserve Maintains Interest Rates, Markets React Positively',
            'description': 'The Federal Reserve announced it will maintain benchmark interest rates at 5.25%-5.50%, in line with market expectations. Major US stock indices rose collectively.',
            'url': 'https://example.com/news1',
            'source': 'Wall Street Journal',
            'published_at': (datetime.now() - timedelta(hours=2)).isoformat(),
            'category': 'Ë¥ßÂ∏ÅÊîøÁ≠ñ',
            'country': 'USA',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/c114dc3c-72a6-44d4-93a8-b97d0608ad58/image_1761210122_4_1.png',
            'fetched_at': datetime.now().isoformat()
        },
        {
            'id': 2,
            'title': 'China GDP Growth Exceeds Expectations, Strong Economic Recovery',
            'description': 'China Q3 GDP grew 5.2% year-on-year, exceeding market expectations of 4.8%. Both consumption and investment showed improvement.',
            'url': 'https://example.com/news2',
            'source': 'Reuters',
            'published_at': (datetime.now() - timedelta(hours=5)).isoformat(),
            'category': 'ÁªèÊµéÊï∞ÊçÆ',
            'country': 'China',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/5f2cdbe0-ff90-4dd6-847d-f845db12d841/image_1761210122_6_1.jpg',
            'fetched_at': datetime.now().isoformat()
        },
        {
            'id': 3,
            'title': 'European Central Bank Hints at Possible Rate Cut, Euro Under Pressure',
            'description': 'ECB President Lagarde indicated the central bank may consider rate cuts if inflation continues to decline. EUR/USD fell 0.8%.',
            'url': 'https://example.com/news3',
            'source': 'Financial Times',
            'published_at': (datetime.now() - timedelta(hours=8)).isoformat(),
            'category': 'Ë¥ßÂ∏ÅÊîøÁ≠ñ',
            'country': 'EU',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/d73313bb-1da1-4247-8ca6-de693c674018/image_1761210124_5_1.png',
            'fetched_at': datetime.now().isoformat()
        },
        {
            'id': 4,
            'title': 'Global Tech Stocks Rally on AI Breakthrough News',
            'description': 'Global tech stocks rose collectively on AI technology breakthrough news. Nvidia, Microsoft and other tech giants hit record highs.',
            'url': 'https://example.com/news4',
            'source': 'CNBC',
            'published_at': (datetime.now() - timedelta(hours=12)).isoformat(),
            'category': 'ËÇ°Â∏ÇÂä®ÊÄÅ',
            'country': 'Global',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/e21eb98a-5751-440c-8dc6-b04471809fc8/image_1761210124_7_1.png',
            'fetched_at': datetime.now().isoformat()
        },
        {
            'id': 5,
            'title': 'Oil Prices Drop Sharply on Global Growth Concerns',
            'description': 'International oil prices fell more than 3% in a single day on concerns about slowing global economic growth. Energy stocks declined broadly.',
            'url': 'https://example.com/news5',
            'source': 'Bloomberg',
            'published_at': (datetime.now() - timedelta(hours=18)).isoformat(),
            'category': 'Â§ßÂÆóÂïÜÂìÅ',
            'country': 'Global',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/c114dc3c-72a6-44d4-93a8-b97d0608ad58/image_1761210122_4_1.png',
            'fetched_at': datetime.now().isoformat()
        },
        {
            'id': 6,
            'title': 'UK Inflation Rate Falls to 2.5%, Below Expectations',
            'description': 'UK CPI rose 2.5% year-on-year in October, below market expectations of 2.8%. Expectations for Bank of England rate cuts increased.',
            'url': 'https://example.com/news6',
            'source': 'BBC',
            'published_at': (datetime.now() - timedelta(hours=24)).isoformat(),
            'category': 'ÁªèÊµéÊï∞ÊçÆ',
            'country': 'UK',
            'image_url': 'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/5f2cdbe0-ff90-4dd6-847d-f845db12d841/image_1761210122_6_1.jpg',
            'fetched_at': datetime.now().isoformat()
        }
    ]
    return mock_news

if __name__ == '__main__':
    main()
