import requests
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict
import time
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

# è…¾è®¯ç›¸å…³å…³é”®è¯
TENCENT_KEYWORDS = [
    'Tencent', 'è…¾è®¯', 'WeChat', 'å¾®ä¿¡', 'QQ', 
    'Tencent Cloud', 'è…¾è®¯äº‘', 'Tencent Games', 'è…¾è®¯æ¸¸æˆ',
    'Honor of Kings', 'ç‹è€…è£è€€', 'PUBG Mobile', 'å’Œå¹³ç²¾è‹±',
    'Tencent Music', 'è…¾è®¯éŸ³ä¹', 'Tencent Video', 'è…¾è®¯è§†é¢‘',
    'Tencent Holdings', '00700.HK', 'TCEHY'
]

# ç›¸å…³è¡Œä¸šå…³é”®è¯
INDUSTRY_KEYWORDS = [
    'gaming', 'game', 'æ¸¸æˆ', 'esports', 'ç”µç«',
    'social media', 'ç¤¾äº¤åª’ä½“', 'messaging', 'å³æ—¶é€šè®¯',
    'cloud computing', 'äº‘è®¡ç®—', 'fintech', 'é‡‘èç§‘æŠ€',
    'digital payment', 'æ•°å­—æ”¯ä»˜', 'streaming', 'æµåª’ä½“',
    'AI', 'artificial intelligence', 'äººå·¥æ™ºèƒ½',
    'metaverse', 'å…ƒå®‡å®™', 'VR', 'AR'
]

# RSSè®¢é˜…æºï¼ˆèšç„¦ç§‘æŠ€å’Œé‡‘èï¼‰
RSS_FEEDS = [
    {
        'url': 'https://feeds.bbci.co.uk/news/technology/rss.xml',
        'source': 'BBC Technology',
        'country': 'UK'
    },
    {
        'url': 'https://www.cnbc.com/id/19854910/device/rss/rss.html',
        'source': 'CNBC Technology',
        'country': 'USA'
    },
    {
        'url': 'https://feeds.reuters.com/reuters/technologyNews',
        'source': 'Reuters Technology',
        'country': 'Global'
    }
]

def fetch_from_google_news_tencent() -> List[Dict]:
    """ä»Google Newsè·å–è…¾è®¯ç›¸å…³æ–°é—»"""
    news_list = []
    
    try:
        # æœç´¢è…¾è®¯ç›¸å…³æ–°é—»
        search_query = 'Tencent OR è…¾è®¯ OR WeChat OR å¾®ä¿¡'
        url = f'https://news.google.com/rss/search?q={search_query}&hl=en-US&gl=US&ceid=US:en'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
            
            for item in items[:15]:
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text().strip()
                    
                    # è½¬æ¢æ—¥æœŸæ ¼å¼
                    try:
                        if pub_date:
                            pub_date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                            pub_date = pub_date_obj.isoformat()
                    except:
                        pub_date = datetime.now().isoformat()
                    
                    news_item = {
                        'id': abs(hash(link)) % (10 ** 8),
                        'title': title,
                        'description': description[:300] if description else '',
                        'url': link,
                        'source': 'Google News',
                        'published_at': pub_date,
                        'category': categorize_tencent_news(title + ' ' + description),
                        'country': 'Global',
                        'image_url': get_placeholder_image(),
                        'fetched_at': datetime.now().isoformat(),
                        'relevance_score': calculate_relevance(title + ' ' + description)
                    }
                    
                    news_list.append(news_item)
                except Exception as e:
                    print(f"è§£æGoogle Newsé¡¹ç›®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            print(f"ä» Google News è·å–åˆ° {len(news_list)} æ¡è…¾è®¯ç›¸å…³æ–°é—»")
            
    except Exception as e:
        print(f"è·å–Google Newså¤±è´¥: {str(e)}")
    
    return news_list

def fetch_from_rss(feed_url: str, source: str, country: str) -> List[Dict]:
    """ä»RSSè®¢é˜…æºè·å–æ–°é—»å¹¶ç­›é€‰è…¾è®¯ç›¸å…³"""
    news_list = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(feed_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
            
            for item in items[:20]:
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    # è®¡ç®—ç›¸å…³æ€§
                    text = title + ' ' + description
                    relevance = calculate_relevance(text)
                    
                    # åªä¿ç•™ç›¸å…³æ€§è¾ƒé«˜çš„æ–°é—»
                    if relevance < 1:
                        continue
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text().strip()
                    
                    # è½¬æ¢æ—¥æœŸæ ¼å¼
                    try:
                        if pub_date:
                            pub_date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                            pub_date = pub_date_obj.isoformat()
                    except:
                        pub_date = datetime.now().isoformat()
                    
                    news_item = {
                        'id': abs(hash(link)) % (10 ** 8),
                        'title': title,
                        'description': description[:300] if description else '',
                        'url': link,
                        'source': source,
                        'published_at': pub_date,
                        'category': categorize_tencent_news(text),
                        'country': country,
                        'image_url': get_placeholder_image(),
                        'fetched_at': datetime.now().isoformat(),
                        'relevance_score': relevance
                    }
                    
                    news_list.append(news_item)
                except Exception as e:
                    print(f"è§£æRSSé¡¹ç›®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            print(f"ä» {source} è·å–åˆ° {len(news_list)} æ¡ç›¸å…³æ–°é—»")
            
    except Exception as e:
        print(f"è·å–RSSè®¢é˜…æºå¤±è´¥ ({source}): {str(e)}")
    
    return news_list

def calculate_relevance(text: str) -> int:
    """è®¡ç®—æ–°é—»ä¸è…¾è®¯çš„ç›¸å…³æ€§åˆ†æ•°"""
    text_lower = text.lower()
    score = 0
    
    # è…¾è®¯ç›´æ¥ç›¸å…³
    for keyword in TENCENT_KEYWORDS:
        if keyword.lower() in text_lower:
            score += 3
    
    # è¡Œä¸šç›¸å…³
    for keyword in INDUSTRY_KEYWORDS:
        if keyword.lower() in text_lower:
            score += 1
    
    return score

def categorize_tencent_news(text: str) -> str:
    """æ ¹æ®æ–‡æœ¬å†…å®¹åˆ†ç±»è…¾è®¯ç›¸å…³æ–°é—»"""
    text_lower = text.lower()
    
    if any(word in text_lower for word in ['game', 'gaming', 'æ¸¸æˆ', 'esports', 'ç”µç«', 'honor of kings', 'pubg']):
        return 'æ¸¸æˆä¸šåŠ¡'
    elif any(word in text_lower for word in ['wechat', 'å¾®ä¿¡', 'qq', 'social', 'ç¤¾äº¤', 'messaging']):
        return 'ç¤¾äº¤å¹³å°'
    elif any(word in text_lower for word in ['cloud', 'äº‘è®¡ç®—', 'tencent cloud', 'è…¾è®¯äº‘']):
        return 'äº‘æœåŠ¡'
    elif any(word in text_lower for word in ['fintech', 'é‡‘èç§‘æŠ€', 'payment', 'æ”¯ä»˜', 'wepay']):
        return 'é‡‘èç§‘æŠ€'
    elif any(word in text_lower for word in ['music', 'éŸ³ä¹', 'video', 'è§†é¢‘', 'streaming', 'æµåª’ä½“']):
        return 'æ•°å­—å†…å®¹'
    elif any(word in text_lower for word in ['ai', 'artificial intelligence', 'äººå·¥æ™ºèƒ½', 'machine learning']):
        return 'AIæŠ€æœ¯'
    elif any(word in text_lower for word in ['stock', 'share', 'è‚¡ä»·', 'market cap', 'å¸‚å€¼', 'earnings', 'è´¢æŠ¥']):
        return 'è‚¡å¸‚è¡¨ç°'
    elif any(word in text_lower for word in ['regulation', 'ç›‘ç®¡', 'policy', 'æ”¿ç­–', 'government', 'æ”¿åºœ']):
        return 'æ”¿ç­–ç›‘ç®¡'
    elif any(word in text_lower for word in ['metaverse', 'å…ƒå®‡å®™', 'vr', 'ar', 'virtual reality']):
        return 'å…ƒå®‡å®™'
    elif any(word in text_lower for word in ['investment', 'æŠ•èµ„', 'acquisition', 'æ”¶è´­', 'partnership', 'åˆä½œ']):
        return 'æŠ•èµ„å¹¶è´­'
    else:
        return 'ç»¼åˆåŠ¨æ€'

def get_placeholder_image() -> str:
    """è·å–å ä½å›¾ç‰‡"""
    images = [
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/c114dc3c-72a6-44d4-93a8-b97d0608ad58/image_1761210122_4_1.png',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/5f2cdbe0-ff90-4dd6-847d-f845db12d841/image_1761210122_6_1.jpg',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/d73313bb-1da1-4247-8ca6-de693c674018/image_1761210124_5_1.png',
        'https://zhiyan-ai-agent-with-1258344702.cos.ap-guangzhou.tencentcos.cn/with/e21eb98a-5751-440c-8dc6-b04471809fc8/image_1761210124_7_1.png'
    ]
    import random
    return random.choice(images)

def fetch_tencent_news() -> List[Dict]:
    """ä»å¤šä¸ªæ¥æºè·å–è…¾è®¯ç›¸å…³æ–°é—»"""
    print("å¼€å§‹è·å–è…¾è®¯ç›¸å…³æ–°é—»...")
    
    all_news = []
    
    # 1. ä»Google Newsè·å–
    print("\n[1/2] ä» Google News è·å–è…¾è®¯æ–°é—»...")
    google_news = fetch_from_google_news_tencent()
    all_news.extend(google_news)
    time.sleep(2)
    
    # 2. ä»RSSè®¢é˜…æºè·å–
    print("\n[2/2] ä» RSS è®¢é˜…æºè·å–ç›¸å…³æ–°é—»...")
    for feed in RSS_FEEDS:
        try:
            rss_news = fetch_from_rss(feed['url'], feed['source'], feed['country'])
            all_news.extend(rss_news)
            time.sleep(1)
        except Exception as e:
            print(f"RSSè®¢é˜…æº {feed['source']} å¤±è´¥: {str(e)}")
            continue
    
    # å»é‡ï¼ˆåŸºäºURLï¼‰
    seen_urls = set()
    unique_news = []
    for news in all_news:
        if news['url'] not in seen_urls:
            seen_urls.add(news['url'])
            unique_news.append(news)
    
    # æŒ‰ç›¸å…³æ€§å’Œå‘å¸ƒæ—¶é—´æ’åº
    unique_news.sort(key=lambda x: (x.get('relevance_score', 0), x['published_at']), reverse=True)
    
    print(f"\næ€»å…±è·å– {len(unique_news)} æ¡å»é‡åçš„è…¾è®¯ç›¸å…³æ–°é—»")
    return unique_news

def analyze_tencent_investment(news_list: List[Dict]) -> Dict:
    """åˆ†æè…¾è®¯æŠ•èµ„å»ºè®®"""
    print("\nå¼€å§‹è…¾è®¯æŠ•èµ„åˆ†æ...")
    
    # æƒ…æ„Ÿåˆ†æå…³é”®è¯
    positive_keywords = [
        'surge', 'rise', 'gain', 'growth', 'increase', 'boost', 'rally',
        'positive', 'optimistic', 'strong', 'recovery', 'improve', 'beat',
        'up', 'higher', 'advance', 'jump', 'soar', 'climb', 'profit',
        'revenue', 'success', 'innovation', 'breakthrough', 'expansion'
    ]
    
    negative_keywords = [
        'fall', 'drop', 'decline', 'decrease', 'loss', 'crash', 'plunge',
        'negative', 'pessimistic', 'weak', 'recession', 'concern', 'miss',
        'down', 'lower', 'tumble', 'sink', 'slump', 'slide', 'regulation',
        'fine', 'penalty', 'ban', 'restriction', 'lawsuit', 'investigation'
    ]
    
    positive_count = 0
    negative_count = 0
    neutral_count = 0
    key_factors = []
    category_sentiment = {}
    
    for news in news_list[:30]:  # åˆ†ææœ€æ–°30æ¡
        text = (news.get('title', '') + ' ' + news.get('description', '')).lower()
        category = news.get('category', 'ç»¼åˆåŠ¨æ€')
        
        pos_score = sum(1 for word in positive_keywords if word in text)
        neg_score = sum(1 for word in negative_keywords if word in text)
        
        # ç»Ÿè®¡åˆ†ç±»æƒ…æ„Ÿ
        if category not in category_sentiment:
            category_sentiment[category] = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        if pos_score > neg_score:
            positive_count += 1
            category_sentiment[category]['positive'] += 1
            if len(key_factors) < 8:
                key_factors.append({
                    'type': 'positive',
                    'category': category,
                    'title': news['title'][:80]
                })
        elif neg_score > pos_score:
            negative_count += 1
            category_sentiment[category]['negative'] += 1
            if len(key_factors) < 8:
                key_factors.append({
                    'type': 'negative',
                    'category': category,
                    'title': news['title'][:80]
                })
        else:
            neutral_count += 1
            category_sentiment[category]['neutral'] += 1
    
    # è®¡ç®—æŠ•èµ„æ¸©åº¦åˆ†æ•°
    total = positive_count + negative_count + neutral_count
    if total > 0:
        temperature_score = ((positive_count * 1.0 + neutral_count * 0.5) / total) * 100
    else:
        temperature_score = 50.0
    
    # ç”ŸæˆæŠ•èµ„å»ºè®®
    investment_advice = generate_investment_advice(
        temperature_score, 
        positive_count, 
        negative_count, 
        neutral_count,
        category_sentiment
    )
    
    # ç¡®å®šæƒ…ç»ª
    if temperature_score >= 70:
        sentiment = "ä¹è§‚"
        sentiment_emoji = "ğŸ˜Š"
    elif temperature_score >= 50:
        sentiment = "ä¸­æ€§"
        sentiment_emoji = "ğŸ˜"
    else:
        sentiment = "è°¨æ…"
        sentiment_emoji = "ğŸ˜Ÿ"
    
    # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
    categories_distribution = {}
    for news in news_list:
        cat = news.get('category', 'ç»¼åˆåŠ¨æ€')
        categories_distribution[cat] = categories_distribution.get(cat, 0) + 1
    
    analysis_result = {
        'temperature_score': round(temperature_score, 1),
        'sentiment': sentiment,
        'sentiment_emoji': sentiment_emoji,
        'investment_advice': investment_advice,
        'key_factors': key_factors,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'neutral_count': neutral_count,
        'analyzed_news_count': len(news_list),
        'analyzed_at': datetime.now().isoformat(),
        'categories_distribution': categories_distribution,
        'category_sentiment': category_sentiment
    }
    
    print(f"åˆ†æå®Œæˆ: æ¸©åº¦={temperature_score:.1f}, æƒ…ç»ª={sentiment}")
    return analysis_result

def generate_investment_advice(score, positive, negative, neutral, category_sentiment):
    """ç”Ÿæˆè¯¦ç»†çš„æŠ•èµ„å»ºè®®"""
    
    advice = {
        'overall_rating': '',
        'risk_level': '',
        'recommendation': '',
        'detailed_analysis': '',
        'key_opportunities': [],
        'key_risks': [],
        'action_items': []
    }
    
    if score >= 75:
        advice['overall_rating'] = 'å¼ºçƒˆçœ‹å¥½'
        advice['risk_level'] = 'ä¸­ç­‰é£é™©'
        advice['recommendation'] = 'å»ºè®®å¢æŒ'
        advice['detailed_analysis'] = f"åŸºäºæœ€æ–°{positive + negative + neutral}æ¡æ–°é—»åˆ†æï¼Œè…¾è®¯æ•´ä½“è¡¨ç°ç§¯æï¼ˆç§¯ææ–°é—»{positive}æ¡ï¼Œå æ¯”{positive/(positive+negative+neutral)*100:.1f}%ï¼‰ã€‚å¤šé¡¹ä¸šåŠ¡æ¿å—å±•ç°å¼ºåŠ²å¢é•¿åŠ¿å¤´ï¼Œå¸‚åœºæƒ…ç»ªä¹è§‚ï¼ŒæŠ•èµ„ä»·å€¼å‡¸æ˜¾ã€‚"
        
        advice['key_opportunities'] = [
            'æ¸¸æˆä¸šåŠ¡æŒç»­å¢é•¿ï¼Œæ–°æ¸¸æˆä¸Šçº¿è¡¨ç°å¼ºåŠ²',
            'äº‘æœåŠ¡å¸‚åœºä»½é¢æ‰©å¤§ï¼Œä¼ä¸šæ•°å­—åŒ–è½¬å‹éœ€æ±‚æ—ºç››',
            'ç¤¾äº¤å¹³å°ç”¨æˆ·æ´»è·ƒåº¦æå‡ï¼Œå¹¿å‘Šæ”¶å…¥å¢é•¿æ½œåŠ›å¤§',
            'AIæŠ€æœ¯åº”ç”¨è½åœ°ï¼Œä¸ºå„ä¸šåŠ¡çº¿èµ‹èƒ½'
        ]
        
        advice['key_risks'] = [
            'ç›‘ç®¡æ”¿ç­–å˜åŒ–å¯èƒ½å½±å“éƒ¨åˆ†ä¸šåŠ¡',
            'å¸‚åœºç«äº‰åŠ å‰§éœ€è¦æŒç»­åˆ›æ–°æŠ•å…¥'
        ]
        
        advice['action_items'] = [
            'å»ºè®®åœ¨å½“å‰ä»·ä½é€‚åº¦å¢æŒï¼Œç›®æ ‡ä»“ä½å¯æå‡è‡³15-20%',
            'é‡ç‚¹å…³æ³¨å­£åº¦è´¢æŠ¥ï¼Œç‰¹åˆ«æ˜¯æ¸¸æˆå’Œäº‘æœåŠ¡æ”¶å…¥',
            'è®¾ç½®æ­¢ç›ˆç‚¹ï¼Œå»ºè®®åœ¨ä¸Šæ¶¨20%ååˆ†æ‰¹è·åˆ©äº†ç»“',
            'é•¿æœŸæŒæœ‰ï¼Œå…³æ³¨3-6ä¸ªæœˆçš„ä¸šç»©è¡¨ç°'
        ]
        
    elif score >= 60:
        advice['overall_rating'] = 'è°¨æ…çœ‹å¥½'
        advice['risk_level'] = 'ä¸­ç­‰é£é™©'
        advice['recommendation'] = 'å»ºè®®æŒæœ‰'
        advice['detailed_analysis'] = f"åŸºäºæœ€æ–°{positive + negative + neutral}æ¡æ–°é—»åˆ†æï¼Œè…¾è®¯æ•´ä½“è¡¨ç°ç¨³å¥ï¼ˆç§¯ææ–°é—»{positive}æ¡ï¼Œæ¶ˆææ–°é—»{negative}æ¡ï¼‰ã€‚è™½ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œä½†æ ¸å¿ƒä¸šåŠ¡åŸºæœ¬é¢è‰¯å¥½ï¼Œå…·å¤‡ä¸­é•¿æœŸæŠ•èµ„ä»·å€¼ã€‚"
        
        advice['key_opportunities'] = [
            'æ ¸å¿ƒä¸šåŠ¡ä¿æŒç¨³å®šå¢é•¿',
            'æ–°ä¸šåŠ¡å¸ƒå±€é€æ­¥æ˜¾ç°æˆæ•ˆ',
            'æŠ€æœ¯åˆ›æ–°æŒç»­æ¨è¿›'
        ]
        
        advice['key_risks'] = [
            'è¡Œä¸šç«äº‰å‹åŠ›å¢å¤§',
            'ç›‘ç®¡ç¯å¢ƒå­˜åœ¨ä¸ç¡®å®šæ€§',
            'éƒ¨åˆ†ä¸šåŠ¡å¢é•¿æ”¾ç¼“'
        ]
        
        advice['action_items'] = [
            'å»ºè®®ç»´æŒå½“å‰ä»“ä½ï¼Œè§‚å¯Ÿåç»­å‘å±•',
            'å¯†åˆ‡å…³æ³¨æ”¿ç­–åŠ¨å‘å’Œç«äº‰æ ¼å±€å˜åŒ–',
            'è®¾ç½®æ­¢æŸç‚¹ï¼Œå»ºè®®åœ¨ä¸‹è·Œ15%æ—¶è€ƒè™‘å‡ä»“',
            'ç­‰å¾…æ›´æ˜ç¡®çš„æŠ•èµ„ä¿¡å·å†åšè°ƒæ•´'
        ]
        
    elif score >= 45:
        advice['overall_rating'] = 'ä¸­æ€§è§‚æœ›'
        advice['risk_level'] = 'ä¸­é«˜é£é™©'
        advice['recommendation'] = 'å»ºè®®è§‚æœ›'
        advice['detailed_analysis'] = f"åŸºäºæœ€æ–°{positive + negative + neutral}æ¡æ–°é—»åˆ†æï¼Œè…¾è®¯å½“å‰é¢ä¸´è¾ƒå¤šä¸ç¡®å®šæ€§ï¼ˆç§¯ææ–°é—»{positive}æ¡ï¼Œæ¶ˆææ–°é—»{negative}æ¡ï¼‰ã€‚å¸‚åœºæƒ…ç»ªåè°¨æ…ï¼Œå»ºè®®ä¿æŒè§‚æœ›æ€åº¦ï¼Œç­‰å¾…æ›´æ¸…æ™°çš„æ–¹å‘ä¿¡å·ã€‚"
        
        advice['key_opportunities'] = [
            'ä¼°å€¼å¤„äºç›¸å¯¹åˆç†åŒºé—´',
            'é•¿æœŸåŸºæœ¬é¢ä»ç„¶ç¨³å›º'
        ]
        
        advice['key_risks'] = [
            'çŸ­æœŸä¸šç»©å‹åŠ›è¾ƒå¤§',
            'ç›‘ç®¡æ”¿ç­–å½±å“æŒç»­',
            'å¸‚åœºç«äº‰åŠ å‰§',
            'å®è§‚ç»æµç¯å¢ƒä¸ç¡®å®š'
        ]
        
        advice['action_items'] = [
            'å»ºè®®æš‚æ—¶è§‚æœ›ï¼Œä¸å»ºè®®æ–°å¢ä»“ä½',
            'å·²æŒæœ‰è€…å¯è€ƒè™‘å‡æŒè‡³5-10%ä»“ä½',
            'è®¾ç½®ä¸¥æ ¼æ­¢æŸï¼Œå»ºè®®åœ¨ä¸‹è·Œ10%æ—¶æ­¢æŸ',
            'ç­‰å¾…æ˜ç¡®çš„è½¬æœºä¿¡å·å†è€ƒè™‘å…¥åœº'
        ]
        
    else:
        advice['overall_rating'] = 'è°¨æ…çœ‹ç©º'
        advice['risk_level'] = 'é«˜é£é™©'
        advice['recommendation'] = 'å»ºè®®å‡æŒ'
        advice['detailed_analysis'] = f"åŸºäºæœ€æ–°{positive + negative + neutral}æ¡æ–°é—»åˆ†æï¼Œè…¾è®¯å½“å‰é¢ä¸´è¾ƒå¤§æŒ‘æˆ˜ï¼ˆæ¶ˆææ–°é—»{negative}æ¡ï¼Œå æ¯”{negative/(positive+negative+neutral)*100:.1f}%ï¼‰ã€‚å¤šé¡¹è´Ÿé¢å› ç´ å åŠ ï¼Œå¸‚åœºæƒ…ç»ªæ‚²è§‚ï¼Œå»ºè®®è°¨æ…å¯¹å¾…ï¼Œé€‚åº¦é™ä½ä»“ä½ã€‚"
        
        advice['key_opportunities'] = [
            'ä¼°å€¼å¯èƒ½å·²ç»åæ˜ éƒ¨åˆ†è´Ÿé¢å› ç´ ',
            'å±æœºä¸­å¯èƒ½å­•è‚²è½¬æœº'
        ]
        
        advice['key_risks'] = [
            'ç›‘ç®¡å‹åŠ›æŒç»­åŠ å¤§',
            'æ ¸å¿ƒä¸šåŠ¡å¢é•¿å—é˜»',
            'å¸‚åœºä»½é¢æµå¤±é£é™©',
            'æŠ•èµ„è€…ä¿¡å¿ƒä¸è¶³',
            'è‚¡ä»·å¯èƒ½ç»§ç»­æ‰¿å‹'
        ]
        
        advice['action_items'] = [
            'å»ºè®®å‡æŒè‡³3-5%ä»¥ä¸‹ä»“ä½æˆ–æ¸…ä»“',
            'é¿å…æŠ„åº•ï¼Œç­‰å¾…æ˜ç¡®çš„åº•éƒ¨ä¿¡å·',
            'å…³æ³¨æ”¿ç­–é¢å’ŒåŸºæœ¬é¢çš„é‡å¤§å˜åŒ–',
            'å¯è€ƒè™‘è½¬å‘å…¶ä»–æ›´ç¨³å¥çš„æŠ•èµ„æ ‡çš„'
        ]
    
    return advice

def save_data(news_list: List[Dict], analysis: Dict):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    # ç¡®ä¿dataç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    # ä¿å­˜æ–°é—»æ•°æ®
    news_data = {
        'updated_at': datetime.now().isoformat(),
        'total_count': len(news_list),
        'news': news_list
    }
    
    with open('data/tencent_news.json', 'w', encoding='utf-8') as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    
    print(f"\næ–°é—»æ•°æ®å·²ä¿å­˜åˆ° data/tencent_news.json")
    
    # ä¿å­˜åˆ†æç»“æœ
    with open('data/tencent_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ° data/tencent_analysis.json")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("è…¾è®¯æ–°é—»çˆ¬å–ä¸æŠ•èµ„åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # è·å–æ–°é—»
        news_list = fetch_tencent_news()
        
        if not news_list:
            print("\nè­¦å‘Š: æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            news_list = generate_mock_data()
        
        # AIåˆ†æ
        analysis = analyze_tencent_investment(news_list)
        
        # ä¿å­˜æ•°æ®
        save_data(news_list, analysis)
        
        print("\n" + "=" * 60)
        print("ä»»åŠ¡å®Œæˆ!")
        print(f"- è·å–æ–°é—»: {len(news_list)} æ¡")
        print(f"- æŠ•èµ„æ¸©åº¦: {analysis['temperature_score']}")
        print(f"- æŠ•èµ„å»ºè®®: {analysis['investment_advice']['recommendation']}")
        print("=" * 60)
        
    except Exception as e:
        print(f"\né”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

def generate_mock_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
    mock_news = [
        {
            'id': 1,
            'title': 'Tencent Reports Strong Q3 Earnings, Gaming Revenue Surges 20%',
            'description': 'Tencent Holdings reported better-than-expected third-quarter earnings, with gaming revenue showing robust growth of 20% year-over-year.',
            'url': 'https://example.com/news1',
            'source': 'Reuters',
            'published_at': datetime.now().isoformat(),
            'category': 'è‚¡å¸‚è¡¨ç°',
            'country': 'China',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 5
        },
        {
            'id': 2,
            'title': 'WeChat Introduces New AI-Powered Features for Business Users',
            'description': 'Tencent\'s WeChat platform launches innovative AI features aimed at enhancing business communication and customer service.',
            'url': 'https://example.com/news2',
            'source': 'TechCrunch',
            'published_at': datetime.now().isoformat(),
            'category': 'AIæŠ€æœ¯',
            'country': 'Global',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 4
        },
        {
            'id': 3,
            'title': 'Tencent Cloud Expands International Presence with New Data Centers',
            'description': 'Tencent Cloud announces expansion plans with new data centers in Southeast Asia and Europe to compete with AWS and Azure.',
            'url': 'https://example.com/news3',
            'source': 'Bloomberg',
            'published_at': datetime.now().isoformat(),
            'category': 'äº‘æœåŠ¡',
            'country': 'Global',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 4
        },
        {
            'id': 4,
            'title': 'Chinese Regulators Approve New Tencent Game Titles',
            'description': 'China\'s gaming regulator approves several new game titles from Tencent, signaling a more favorable regulatory environment.',
            'url': 'https://example.com/news4',
            'source': 'CNBC',
            'published_at': datetime.now().isoformat(),
            'category': 'æ”¿ç­–ç›‘ç®¡',
            'country': 'China',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 5
        },
        {
            'id': 5,
            'title': 'Tencent Music Entertainment Sees User Growth Amid Competition',
            'description': 'Despite fierce competition, Tencent Music reports steady user growth and improved monetization strategies.',
            'url': 'https://example.com/news5',
            'source': 'Financial Times',
            'published_at': datetime.now().isoformat(),
            'category': 'æ•°å­—å†…å®¹',
            'country': 'China',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 3
        },
        {
            'id': 6,
            'title': 'Tencent Invests in Metaverse Startups, Eyes Future Growth',
            'description': 'Tencent announces strategic investments in several metaverse and VR technology startups as part of its long-term growth strategy.',
            'url': 'https://example.com/news6',
            'source': 'TechNode',
            'published_at': datetime.now().isoformat(),
            'category': 'å…ƒå®‡å®™',
            'country': 'Global',
            'image_url': get_placeholder_image(),
            'fetched_at': datetime.now().isoformat(),
            'relevance_score': 4
        }
    ]
    return mock_news

if __name__ == '__main__':
    main()

